# 每日复盘与总结

## 2023.10.28

### 算法

一开始回顾了快排和归并排序的算法实现。快排和归并都用到了递归，也就是分治的思想
快排选取的比较值是随机选取的，在这个算法中选的是$q[l+r>>1]$处的值
归并是划分成两个区间后，对这两个区间中的值进行排序，递归地对全局进行排序

**特别注意的是循环条件以及等号是否能够选取。**

$DFS$和$BFS$都是搜索算法，区别是$BFS$能够找到最优$^{[1]}$，如果只是求解而不考虑最优时可以用$BFS$.

树与图的存储一般使用邻接表进行存储的，但是一般用数组模拟邻接表。

输入数据量为$10^6$时，考虑`scanf`，否则`cin`的误差可忽略。

### 语音深度学习

根据[语音信号处理的深度学习入门](https://zhuanlan.zhihu.com/p/386467252)，再次梳理了语音深度学习的基础知识部分，存放在*E:\HQU\科研\语音伪造检测\notes.md*

关键是成功运行了一个获取Mel谱的代码，总结一下步骤就是
$$
头尾静音消除→预加重→STFT→获取幅度谱→获取Mel谱
$$
学习了李沐的深度学习中第六章 卷积神经网络的相关内容，浅浅说一下自己的思考：

之前的DNN在CNN没有出现之前，一直都是使用MLP进行数据处理，这对于早期的深度学习而言并没有太大问题，但会遇到以下两个方面的挑战：$(1)$倘若数据量不断增加时，MLP的参数便会不断增长.$(2)$对于图像类数据而言，MLP的参数同样会日益膨胀.尤其是对于问题$(2)$而言，当时的人们思考是否能够精简模型的学习参数，对于图像而言，还要考虑到最重要的两个性质:$(1)$平移不变性$(2)$局部性，因此卷积运算$^{[2]}$便巧妙地与神经网络结合起来。



$————————————————————————————————————————————————————————$

$[1]$ $BFS$找到最优仅适用于权重相同的情况，权重不同考虑最短路如$Dijkstra$，同时如果没有环路，也可以使用$DP$。

$[2]$ 卷积运算常被定义为:$(f*g)(t)= \int_{-\infty}^{+\infty}f(x)g(t-x)dt$.在CNN中，卷积运算与该式中所写的有着微妙的不同，但从本质上来讲是一回事。此处不展开赘述，请参考b站视频或其它资料.



## 2023.10.31

### Pytorch学习

**以后数据集可以去Kaggle上找。。下载完了之后再放到data文件夹里。。**



可以查询[TORCH.UTILS.DATA](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)来查看相关API

- `torch.utils.data.Dataset`	[link](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)
  - 简单来说是用来读取数据集的类
  - 支持自定义数据集，不过需要重写`__init__(self,...)`,`__len__(self)`,`__getitem__(self,idx)`方法
  - `__getitem__()`的魔法函数使得实例化的对象支持下标访问

- ``torch.utils.data.DataLoader`    [link](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
  - 用来加载数据集，并对数据集进行一系列操作
  - 实例化的对象是可迭代的，因此可以用于训练，其余相关参数详见link
- `torch.nn`    link







### 论文阅读

#### A study on data augmentation in voice anti-spoofing

Ariel Cohen ∗,**, Inbal Rimon**, Eran Aflalo, Haim H. Permuter

Speech Communication

##### 摘要部分：

在本文中，我们对数据增强技术如何改进合成或欺骗的音频检测进行了深入的研究。具体地说，我们提出了处理**信道变化、不同的音频压缩、不同的带宽、不可见的欺骗攻击**的方法。这些挑战，都被证明显著降低基于音频的系统和反欺骗系统的性能。我们的研究结果是基于ASVspoof 2021挑战，在逻辑访问（LA）和深度伪造（DF）类别中。我们的研究是**以数据为中心**的，这意味着模型是固定的，我们通过操纵数据显著地改善了结果。我们介绍了两种形式的数据增强-测向部分的压缩增强，以及LA部分的压缩和信道增强。此外，还介绍了一种新型的在线数据增强方法，即规格平均。该方法包括将音频特征的**平均值**掩蔽，以提高泛化性。我们最好的单一系统和融合方案在DF类别中都达到了最先进的性能，EER分别为15.46%和14.27%。我们对LA任务的最佳系统将最佳基线EER降低了50%，最小t-DCF降低了16%。我们处理来自各种分布的欺骗数据的技术可以被复制，并可以帮助反欺骗和基于语音的系统增强他们的结果。

Keywords：ASVspoof 2021	Audio data augmentation	Data-centric AI	SpecAugment 	Voice anti spoofing	Voice deep fake

##### 遇到的问题

语音伪造检测所面临的问题：

1. **压缩(Compression)**	有损压缩可能会降低ASV的性能
2. **信道效应(Channel effects)**	通过信道传输音频文件所遇到的如丢包、噪声可能会对ASV造成影响
3. **频带差异与滤波(Bandwidth differences and filtering)**	对于不同带宽的编解码器，可能会造成高频信息的丢失，会可能包含伪造检测的关键信息
4. **未见攻击(Unseen spoof attacks)**	对于未知的攻击，ASV的处理性能有所下降

##### 以数据为中心

在这种方法中，模型是固定的，并在数据集中不断地进行更改/改进，以使结果最大化，本文即采用这种方法，采取一系列数据增强以及特征设计的策略对数据加以处理。

##### 动机

1.2.1.2中提到的问题深刻的影响到了ASV系统的性能，激发了相关工作。





##### 疑惑

LFCC、对数谱争取自己实现一下

预加重

什么是receptive field

consecutive frequency bins or coeddicients?

## 2023.11.02

### Pytorch学习

可以查询[TORCH.UTILS.DATA](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)来查看相关API

- `torch.utils.data.Dataset`	[link](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)
  - 简单来说是用来读取数据集的类
  - 支持自定义数据集，不过需要重写`__init__(self,...)`,`__len__(self)`,`__getitem__(self,idx)`方法
  - `__getitem__()`的魔法函数使得实例化的对象支持下标访问

- ``torch.utils.data.DataLoader`    [link](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
  - 用来加载数据集，并对数据集进行一系列操作
  - 实例化的对象是可迭代的，因此可以用于训练，其余相关参数详见link
- `torch.nn.Module`    [link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)
  - 用来构建神经网络模型，**是所有神经网络单元（neural network modules）的基类**，同时在`__call__(self,*input,**kwargs)`方法中，调用了`forward(self,input)`函数
  - 因此基于`nn.Module`子类的实例，比如`nn.Linear()`的实例，在写法上类似于函数的写法，可以接收一个参数，比如`m=nn.Linear(20,30)  output=m(input)`，但实际上这个是调用的`__call__(input)`方法，再来调用`forward(input)`函数.
  - 所以在自定义神经网络时，只需要重写`__init__(self)`方法和`forward(self,input)`方法即可。

- `torch.nn.Sequential(*args:Module)`    [link](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)
  - 用来当作网络层的便捷写法

